{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pranjalipilankar/Moon-Map/blob/main/train2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#uncomment for colab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4UvRs_YpjRl",
        "outputId": "1729656f-ee40-4fec-e31b-f4aff240d3b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd drive/MyDrive/Moon\\ Mapping/Models/Moon-Mapping/AI\\ Models/SwinIR/\n",
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Y-jToNvpme1",
        "outputId": "c13ba59c-2e0c-440e-83ed-2f6c17650eb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/19TyNbSyd7i1igZMVw4YRX5xonl55yZTb/Moon Mapping/Models/Moon-Mapping/AI Models/SwinIR\n",
            " architecture.gdoc                                         \u001b[0m\u001b[01;34mlightning_logs\u001b[0m/\n",
            " architecture.txt                                          main_test_swinir.py\n",
            " check.png                                                 mismatch.txt\n",
            " \u001b[01;34mCheckpoints\u001b[0m/                                              \u001b[01;34mmodels\u001b[0m/\n",
            " cog.yaml                                                  \u001b[01;34mmodel_zoo\u001b[0m/\n",
            " \u001b[01;34mDataSet\u001b[0m/                                                 'Moon mapping.pdf'\n",
            " \u001b[01;34mDataSet_Gray_8x\u001b[0m/                                          network_swinir_state_dict.txt\n",
            " \u001b[01;34mDataSet_Gray_High\u001b[0m/                                        predict.py\n",
            "\u001b[01;34m'DataSet_ Grayscale'\u001b[0m/                                      README.md\n",
            " download-weights.sh                                       \u001b[01;34mtestsets\u001b[0m/\n",
            " \u001b[01;34mexperiments\u001b[0m/                                              \u001b[01;34mtrain\u001b[0m/\n",
            " Extracting-Intersection-Coordinates-Between-Images.pptx   train2.ipynb\n",
            " \u001b[01;34mfigs\u001b[0m/                                                     train.ipynb\n",
            " Introduction-to-SWIN-Transformers.pptx                    \u001b[01;34mutils\u001b[0m/\n",
            " LICENSE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install timm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dfjJ5oZp-cY",
        "outputId": "7416817f-5f4a-4c6d-e4ad-9a37fc7e871f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (1.0.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm) (2.3.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.18.0+cu121)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.1)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.23.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (24.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.12.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->timm) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->timm) (12.5.40)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2024.6.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->timm) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jWr3ei0knqqf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from models.network_swinir import SwinIR as net\n",
        "from utils import util_calculate_psnr_ssim as util\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2UdGXSE-nqqi",
        "outputId": "240770cd-9340-4a09-b2bf-28bb237a4228"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:512: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3587.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SwinIR(\n",
              "  (conv_first): Conv2d(1, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (patch_embed): PatchEmbed(\n",
              "    (norm): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (patch_unembed): PatchUnEmbed()\n",
              "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
              "  (layers): ModuleList(\n",
              "    (0): RSTB(\n",
              "      (residual_group): BasicLayer(\n",
              "        dim=180, input_resolution=(128, 128), depth=6\n",
              "        (blocks): ModuleList(\n",
              "          (0): SwinTransformerBlock(\n",
              "            dim=180, input_resolution=(128, 128), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
              "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              dim=180, window_size=(8, 8), num_heads=6\n",
              "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (softmax): Softmax(dim=-1)\n",
              "            )\n",
              "            (drop_path): Identity()\n",
              "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (1): SwinTransformerBlock(\n",
              "            dim=180, input_resolution=(128, 128), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
              "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              dim=180, window_size=(8, 8), num_heads=6\n",
              "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (softmax): Softmax(dim=-1)\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.003)\n",
              "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (2): SwinTransformerBlock(\n",
              "            dim=180, input_resolution=(128, 128), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
              "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              dim=180, window_size=(8, 8), num_heads=6\n",
              "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (softmax): Softmax(dim=-1)\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.006)\n",
              "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (3): SwinTransformerBlock(\n",
              "            dim=180, input_resolution=(128, 128), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
              "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              dim=180, window_size=(8, 8), num_heads=6\n",
              "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (softmax): Softmax(dim=-1)\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.009)\n",
              "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (4): SwinTransformerBlock(\n",
              "            dim=180, input_resolution=(128, 128), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
              "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              dim=180, window_size=(8, 8), num_heads=6\n",
              "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (softmax): Softmax(dim=-1)\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.011)\n",
              "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (5): SwinTransformerBlock(\n",
              "            dim=180, input_resolution=(128, 128), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
              "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              dim=180, window_size=(8, 8), num_heads=6\n",
              "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (softmax): Softmax(dim=-1)\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.014)\n",
              "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (patch_embed): PatchEmbed()\n",
              "      (patch_unembed): PatchUnEmbed()\n",
              "    )\n",
              "    (1): RSTB(\n",
              "      (residual_group): BasicLayer(\n",
              "        dim=180, input_resolution=(128, 128), depth=6\n",
              "        (blocks): ModuleList(\n",
              "          (0): SwinTransformerBlock(\n",
              "            dim=180, input_resolution=(128, 128), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
              "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              dim=180, window_size=(8, 8), num_heads=6\n",
              "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (softmax): Softmax(dim=-1)\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.017)\n",
              "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (1): SwinTransformerBlock(\n",
              "            dim=180, input_resolution=(128, 128), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
              "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              dim=180, window_size=(8, 8), num_heads=6\n",
              "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (softmax): Softmax(dim=-1)\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.020)\n",
              "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (2): SwinTransformerBlock(\n",
              "            dim=180, input_resolution=(128, 128), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
              "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              dim=180, window_size=(8, 8), num_heads=6\n",
              "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (softmax): Softmax(dim=-1)\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.023)\n",
              "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (3): SwinTransformerBlock(\n",
              "            dim=180, input_resolution=(128, 128), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
              "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              dim=180, window_size=(8, 8), num_heads=6\n",
              "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (softmax): Softmax(dim=-1)\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.026)\n",
              "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (4): SwinTransformerBlock(\n",
              "            dim=180, input_resolution=(128, 128), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
              "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              dim=180, window_size=(8, 8), num_heads=6\n",
              "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (softmax): Softmax(dim=-1)\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.029)\n",
              "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (5): SwinTransformerBlock(\n",
              "            dim=180, input_resolution=(128, 128), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
              "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              dim=180, window_size=(8, 8), num_heads=6\n",
              "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (softmax): Softmax(dim=-1)\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.031)\n",
              "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (patch_embed): PatchEmbed()\n",
              "      (patch_unembed): PatchUnEmbed()\n",
              "    )\n",
              "    (2): RSTB(\n",
              "      (residual_group): BasicLayer(\n",
              "        dim=180, input_resolution=(128, 128), depth=6\n",
              "        (blocks): ModuleList(\n",
              "          (0): SwinTransformerBlock(\n",
              "            dim=180, input_resolution=(128, 128), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
              "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              dim=180, window_size=(8, 8), num_heads=6\n",
              "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (softmax): Softmax(dim=-1)\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.034)\n",
              "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (1): SwinTransformerBlock(\n",
              "            dim=180, input_resolution=(128, 128), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
              "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              dim=180, window_size=(8, 8), num_heads=6\n",
              "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (softmax): Softmax(dim=-1)\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.037)\n",
              "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (2): SwinTransformerBlock(\n",
              "            dim=180, input_resolution=(128, 128), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
              "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              dim=180, window_size=(8, 8), num_heads=6\n",
              "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (softmax): Softmax(dim=-1)\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.040)\n",
              "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (3): SwinTransformerBlock(\n",
              "            dim=180, input_resolution=(128, 128), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
              "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              dim=180, window_size=(8, 8), num_heads=6\n",
              "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (softmax): Softmax(dim=-1)\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.043)\n",
              "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (4): SwinTransformerBlock(\n",
              "            dim=180, input_resolution=(128, 128), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
              "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              dim=180, window_size=(8, 8), num_heads=6\n",
              "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (softmax): Softmax(dim=-1)\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.046)\n",
              "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (5): SwinTransformerBlock(\n",
              "            dim=180, input_resolution=(128, 128), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
              "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              dim=180, window_size=(8, 8), num_heads=6\n",
              "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (softmax): Softmax(dim=-1)\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.049)\n",
              "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (patch_embed): PatchEmbed()\n",
              "      (patch_unembed): PatchUnEmbed()\n",
              "    )\n",
              "    (3): RSTB(\n",
              "      (residual_group): BasicLayer(\n",
              "        dim=180, input_resolution=(128, 128), depth=6\n",
              "        (blocks): ModuleList(\n",
              "          (0): SwinTransformerBlock(\n",
              "            dim=180, input_resolution=(128, 128), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
              "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              dim=180, window_size=(8, 8), num_heads=6\n",
              "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (softmax): Softmax(dim=-1)\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.051)\n",
              "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (1): SwinTransformerBlock(\n",
              "            dim=180, input_resolution=(128, 128), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
              "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              dim=180, window_size=(8, 8), num_heads=6\n",
              "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (softmax): Softmax(dim=-1)\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.054)\n",
              "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (2): SwinTransformerBlock(\n",
              "            dim=180, input_resolution=(128, 128), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
              "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              dim=180, window_size=(8, 8), num_heads=6\n",
              "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (softmax): Softmax(dim=-1)\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.057)\n",
              "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (3): SwinTransformerBlock(\n",
              "            dim=180, input_resolution=(128, 128), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
              "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              dim=180, window_size=(8, 8), num_heads=6\n",
              "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (softmax): Softmax(dim=-1)\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.060)\n",
              "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (4): SwinTransformerBlock(\n",
              "            dim=180, input_resolution=(128, 128), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
              "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              dim=180, window_size=(8, 8), num_heads=6\n",
              "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (softmax): Softmax(dim=-1)\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.063)\n",
              "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (5): SwinTransformerBlock(\n",
              "            dim=180, input_resolution=(128, 128), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
              "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              dim=180, window_size=(8, 8), num_heads=6\n",
              "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (softmax): Softmax(dim=-1)\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.066)\n",
              "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (patch_embed): PatchEmbed()\n",
              "      (patch_unembed): PatchUnEmbed()\n",
              "    )\n",
              "    (4): RSTB(\n",
              "      (residual_group): BasicLayer(\n",
              "        dim=180, input_resolution=(128, 128), depth=6\n",
              "        (blocks): ModuleList(\n",
              "          (0): SwinTransformerBlock(\n",
              "            dim=180, input_resolution=(128, 128), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
              "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              dim=180, window_size=(8, 8), num_heads=6\n",
              "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (softmax): Softmax(dim=-1)\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.069)\n",
              "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (1): SwinTransformerBlock(\n",
              "            dim=180, input_resolution=(128, 128), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
              "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              dim=180, window_size=(8, 8), num_heads=6\n",
              "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (softmax): Softmax(dim=-1)\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.071)\n",
              "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (2): SwinTransformerBlock(\n",
              "            dim=180, input_resolution=(128, 128), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
              "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              dim=180, window_size=(8, 8), num_heads=6\n",
              "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (softmax): Softmax(dim=-1)\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.074)\n",
              "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (3): SwinTransformerBlock(\n",
              "            dim=180, input_resolution=(128, 128), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
              "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              dim=180, window_size=(8, 8), num_heads=6\n",
              "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (softmax): Softmax(dim=-1)\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.077)\n",
              "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (4): SwinTransformerBlock(\n",
              "            dim=180, input_resolution=(128, 128), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
              "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              dim=180, window_size=(8, 8), num_heads=6\n",
              "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (softmax): Softmax(dim=-1)\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.080)\n",
              "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (5): SwinTransformerBlock(\n",
              "            dim=180, input_resolution=(128, 128), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
              "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              dim=180, window_size=(8, 8), num_heads=6\n",
              "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (softmax): Softmax(dim=-1)\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.083)\n",
              "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (patch_embed): PatchEmbed()\n",
              "      (patch_unembed): PatchUnEmbed()\n",
              "    )\n",
              "    (5): RSTB(\n",
              "      (residual_group): BasicLayer(\n",
              "        dim=180, input_resolution=(128, 128), depth=6\n",
              "        (blocks): ModuleList(\n",
              "          (0): SwinTransformerBlock(\n",
              "            dim=180, input_resolution=(128, 128), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
              "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              dim=180, window_size=(8, 8), num_heads=6\n",
              "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (softmax): Softmax(dim=-1)\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.086)\n",
              "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (1): SwinTransformerBlock(\n",
              "            dim=180, input_resolution=(128, 128), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
              "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              dim=180, window_size=(8, 8), num_heads=6\n",
              "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (softmax): Softmax(dim=-1)\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.089)\n",
              "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (2): SwinTransformerBlock(\n",
              "            dim=180, input_resolution=(128, 128), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
              "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              dim=180, window_size=(8, 8), num_heads=6\n",
              "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (softmax): Softmax(dim=-1)\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.091)\n",
              "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (3): SwinTransformerBlock(\n",
              "            dim=180, input_resolution=(128, 128), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
              "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              dim=180, window_size=(8, 8), num_heads=6\n",
              "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (softmax): Softmax(dim=-1)\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.094)\n",
              "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (4): SwinTransformerBlock(\n",
              "            dim=180, input_resolution=(128, 128), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2\n",
              "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              dim=180, window_size=(8, 8), num_heads=6\n",
              "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (softmax): Softmax(dim=-1)\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.097)\n",
              "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (5): SwinTransformerBlock(\n",
              "            dim=180, input_resolution=(128, 128), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2\n",
              "            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              dim=180, window_size=(8, 8), num_heads=6\n",
              "              (qkv): Linear(in_features=180, out_features=540, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=180, out_features=180, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (softmax): Softmax(dim=-1)\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.100)\n",
              "            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=180, out_features=360, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=360, out_features=180, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (patch_embed): PatchEmbed()\n",
              "      (patch_unembed): PatchUnEmbed()\n",
              "    )\n",
              "  )\n",
              "  (norm): LayerNorm((180,), eps=1e-05, elementwise_affine=True)\n",
              "  (conv_after_body): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv_last): Conv2d(180, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "model = net(upscale=1, in_chans=1, img_size=128, window_size=8,\n",
        "            img_range=1., depths=[6, 6, 6, 6, 6, 6], embed_dim=180, num_heads=[6, 6, 6, 6, 6, 6],\n",
        "            mlp_ratio=2, upsampler='', resi_connection='1conv')\n",
        "param_key_g = 'params'\n",
        "\n",
        "model_path = r'/content/drive/MyDrive/Moon Mapping/Models/Moon-Mapping/AI Models/SwinIR/experiments/pretrained_models/004_grayDN_DFWB_s128w8_SwinIR-M_noise50.pth'\n",
        "pretrained_model = torch.load(model_path)\n",
        "model.load_state_dict(pretrained_model[param_key_g] if param_key_g in pretrained_model.keys() else pretrained_model, strict=True)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model.to(device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w20O1hl3nqql"
      },
      "outputs": [],
      "source": [
        "from torchsummary import summary\n",
        "summary(model, (1, 128, 128))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iz4dRzOjnqqm"
      },
      "outputs": [],
      "source": [
        "# container = list(model.children())\n",
        "# # container[7] = torch.nn.ReLU()\n",
        "# model = torch.nn.Sequential(*container)\n",
        "# list(model.children())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlLoOTI4nqqn",
        "outputId": "583f4775-c534-451f-ded4-aae47bce4486"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1, 128, 128])\n",
            "torch.Size([1, 128, 128])\n"
          ]
        }
      ],
      "source": [
        "img = Image.open(r'/content/drive/MyDrive/Moon Mapping/Models/Moon-Mapping/AI Models/SwinIR/DataSet/GT/324.png')\n",
        "img = img.resize((128, 128))\n",
        "img.show()\n",
        "img = transforms.PILToTensor()(img).to(device=device)[0,:,:]\n",
        "img = img.unsqueeze(0).unsqueeze(0).to(dtype=torch.float32)\n",
        "print(img.size())\n",
        "model.eval()\n",
        "img = model(img).to(device='cpu')\n",
        "img = img.squeeze(0)\n",
        "print(img.size())\n",
        "img = transforms.ToPILImage()(img).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "Be1QNIAgnqqo",
        "outputId": "67bd6519-1b56-4fed-96b3-8064e195a197"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-660ff4014b6c>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Checkpoints/model_1_pl_perceptual_loss/epoch=18-train_loss=1.14.ckpt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1023\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUNSAFE_MESSAGE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1025\u001b[0;31m                 return _load(opened_zipfile,\n\u001b[0m\u001b[1;32m   1026\u001b[0m                              \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m                              \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnpicklerWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1446\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1448\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_loaded_sparse_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1414\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1415\u001b[0m             \u001b[0mnbytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumel\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1416\u001b[0;31m             \u001b[0mtyped_storage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_maybe_decode_ascii\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1418\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtyped_storage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1388\u001b[0m         \u001b[0;31m# stop wrapping with TypedStorage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1389\u001b[0m         typed_storage = torch.storage.TypedStorage(\n\u001b[0;32m-> 1390\u001b[0;31m             \u001b[0mwrap_storage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1391\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1392\u001b[0m             _internal=True)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_package_registry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_cuda_deserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_cuda_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_torch_load_uninitialized\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mvalidate_cuda_device\u001b[0;34m(location)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         raise RuntimeError('Attempting to deserialize object on a CUDA '\n\u001b[0m\u001b[1;32m    250\u001b[0m                            \u001b[0;34m'device but torch.cuda.is_available() is False. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m                            \u001b[0;34m'If you are running on a CPU-only machine, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
          ]
        }
      ],
      "source": [
        "import torch\n",
        "ckpt = torch.load('Checkpoints/model_1_pl_perceptual_loss/epoch=18-train_loss=1.14.ckpt')\n",
        "print(ckpt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAXcRnItnqqp",
        "outputId": "2178c3c8-81cb-484f-caad-b8c046093da7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/sukhvansh/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import pytorch_lightning as pl\n",
        "from models.network_swinir import default_model_import\n",
        "import torch\n",
        "import time\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "# %pip install torch-lr-finder\n",
        "from torch_lr_finder import LRFinder\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import torchvision.models as models\n",
        "from PIL import Image\n",
        "import os\n",
        "# %pip install pytorch-msssim\n",
        "from pytorch_msssim import ssim\n",
        "from models.network_swinir import SwinIR as net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6bmzCDtnqqq"
      },
      "outputs": [],
      "source": [
        "class PerceptualLoss(nn.Module):\n",
        "    def __init__(self, resize=True):\n",
        "        super(PerceptualLoss, self).__init__()\n",
        "        # Use pre-trained VGG model (features only)\n",
        "        blocks = []\n",
        "        blocks.append(models.vgg16(pretrained=True).features[:4].eval())\n",
        "        blocks.append(models.vgg16(pretrained=True).features[4:9].eval())\n",
        "        self.blocks = torch.nn.ModuleList(blocks)\n",
        "\n",
        "        # Freeze weights\n",
        "        for block in self.blocks:\n",
        "            for p in block.parameters():\n",
        "                p.requires_grad = False\n",
        "\n",
        "        self.transform = nn.functional.interpolate.Bilinear2d(scale_factor=1.0) if resize else None\n",
        "        self.register_buffer(\"mean\", torch.tensor([0.485]).view(1, 1, 1, 1))\n",
        "        self.register_buffer(\"std\", torch.tensor([0.229]).view(1, 1, 1, 1))\n",
        "\n",
        "    def forward(self, input, target, feature_weights=[1.0, 1.0]):\n",
        "        \"\"\"\n",
        "        Calculates perceptual loss between input and target grayscale images.\n",
        "\n",
        "        Args:\n",
        "            input: Tensor of generated images (N, 1, 256, 256).\n",
        "            target: Tensor of ground truth images (N, 1, 256, 256).\n",
        "            feature_weights: List of weights for each feature layer (default: [1.0, 1.0]).\n",
        "\n",
        "        Returns:\n",
        "            Total perceptual loss (scalar).\n",
        "        \"\"\"\n",
        "\n",
        "        if input.size(1) != 1:\n",
        "            raise ValueError(\"Input must be a grayscale image (channel = 1)\")\n",
        "        if target.size(1) != 1:\n",
        "            raise ValueError(\"Target must be a grayscale image (channel = 1)\")\n",
        "\n",
        "        # Preprocess input and target\n",
        "        input = input.repeat(1, 3, 1, 1)  # Replicate grayscale to 3 channels\n",
        "        target = target.repeat(1, 3, 1, 1)\n",
        "        input = (input - self.mean) / self.std\n",
        "        target = (target - self.mean) / self.std\n",
        "\n",
        "        if self.transform:\n",
        "            input = self.transform(input)\n",
        "            target = self.transform(target)\n",
        "\n",
        "        # Extract features from both images\n",
        "        f_input = []\n",
        "        f_target = []\n",
        "        for block in self.blocks:\n",
        "            input = block(input)\n",
        "            target = block(target)\n",
        "            f_input.append(input)\n",
        "            f_target.append(target)\n",
        "\n",
        "        # Calculate loss for each feature layer\n",
        "        loss = 0.0\n",
        "        for i, (f_in, f_out) in enumerate(zip(f_input, f_target)):\n",
        "            loss += feature_weights[i] * torch.nn.functional.l1_loss(f_in, f_out)\n",
        "\n",
        "        return loss\n",
        "\n",
        "class CombinedLoss(nn.Module):\n",
        "  def __init__(self, perceptual_weight=1.0, mse_weight=1.0, resize=False):\n",
        "    super(CombinedLoss, self).__init__()\n",
        "    self.perceptual_loss = PerceptualLoss(resize=resize)\n",
        "    self.mse_loss = nn.MSELoss()\n",
        "    self.perceptual_weight = perceptual_weight\n",
        "    self.mse_weight = mse_weight\n",
        "\n",
        "  def forward(self, input, target):\n",
        "    # Calculate perceptual and MSE loss\n",
        "    perceptual_loss = self.perceptual_loss(input, target)\n",
        "    mse_loss = self.mse_weight * self.mse_loss(input, target)\n",
        "\n",
        "    # Combine losses with weights\n",
        "    loss = self.perceptual_weight * perceptual_loss + mse_loss\n",
        "    return loss\n",
        "\n",
        "class model1(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = net(upscale=4, in_chans=1, img_size=64, window_size=8,\n",
        "                        img_range=1., depths=[6, 6, 6, 6, 6, 6], embed_dim=180, num_heads=[6, 6, 6, 6, 6, 6],\n",
        "                        mlp_ratio=2, upsampler='nearest+conv', resi_connection='1conv')\n",
        "        self.criterion = CombinedLoss()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=1e-4)\n",
        "        return optimizer\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        # access your optimizers with use_pl_optimizer=False. Default is True\n",
        "        inputs, labels = batch[0], batch[1]\n",
        "        outputs = self.model(inputs)\n",
        "        loss = self.criterion(outputs, labels)\n",
        "        self.log('train_loss', loss)\n",
        "        return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wUmJ2pkLnqqr"
      },
      "outputs": [],
      "source": [
        "model = model1.load_from_checkpoint('Checkpoints/model_1_pl_perceptual_loss/epoch=26-train_loss=1.03.ckpt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3sPtmzdnqqs",
        "outputId": "5228df17-a53d-482e-fe98-878bf114ec5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 1, 64, 64])\n",
            "torch.Size([1, 256, 256])\n"
          ]
        }
      ],
      "source": [
        "img = Image.open('DataSet_Gray_High/Train/324.png')\n",
        "img.show()\n",
        "img = transforms.PILToTensor()(img).to(device='cuda')[0,:,:]\n",
        "img = img.unsqueeze(0).unsqueeze(0).to(dtype=torch.float32)\n",
        "print(img.size())\n",
        "model.eval()\n",
        "img = model.forward(img).to(device='cpu')\n",
        "img = img.squeeze(0)\n",
        "print(img.size())\n",
        "img = transforms.ToPILImage()(img).save('check.png')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}